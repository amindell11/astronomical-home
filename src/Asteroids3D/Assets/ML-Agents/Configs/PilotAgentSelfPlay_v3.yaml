behaviors:
  RLPilot:
    trainer_type: ppo
    
    hyperparameters:
      batch_size: 1024
      buffer_size: 10240
      learning_rate: 2.0e-4          # Reduced from 3.0e-4 for first 100k steps as per plan
      learning_rate_schedule: linear
      beta: 2.0e-3
      beta_schedule: constant
      epsilon: 0.2
      epsilon_schedule: linear
      lambd: 0.95
      num_epoch: 3

    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 2
      vis_encode_type: simple
      
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0

    # Self-play configuration for competitive training with hybrid boundary scheme
    self_play:
      window: 10                        # Number of past policies to keep
      play_against_latest_model_ratio: 0.5  # 50% chance to play against current model
      save_steps: 50000                 # Save policy snapshot every 50k steps  
      swap_steps: 10000                 # Change opponent every 10k steps
      team_change: 500000               # Switch which team is learning every 500k steps

    max_steps: 10000000
    summary_freq: 20000
    time_horizon: 2048                  # Increased from 1024 for longer episodes as per plan
    keep_checkpoints: 5
    checkpoint_interval: 500000

# Randomized environment parameters for robust self-play training
# Supports hybrid boundary scheme with soft pull + distant hard wall
environment_parameters:
  arena_size:
    sampler_type: uniform
    sampler_parameters:
      min_value: 40     # Minimum arena size (challenging) - R_soft = 30m, R_hard = 48m
      max_value: 80     # Maximum arena size (easier) - R_soft = 60m, R_hard = 96m
    
  asteroid_density:
    sampler_type: uniform
    sampler_parameters:
      min_value: 0.4    # Light asteroid field
      max_value: 0.8    # Dense asteroid field
